<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta name="description" content="A layout example that shows off a blog page with a list of posts.">
		<title>A little LUA learner</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css" rel="stylesheet">
		<link rel="stylesheet" href="css/pure/pure-min.css">
		<link rel="stylesheet" href="css/pure/grids-responsive-min.css">
		<link rel="stylesheet" href="styles.css">
    <link rel="shortcut icon" sizes="16x16"  href="favicon.png"/>
	</head>
	<body>

		<div id="layout" class="pure-g">
			<div class="sidebar pure-u-1 pure-u-md-1-4">
				<div class="header">
					<h1 class="brand-title">l5</h1>
					<h2 class="brand-tagline">a little LUA learning library</h2>
					<nav class="nav">
						<ul class="nav-list">
							<li class="nav-item"><a class="pure-button" href="http://github.com/timm/l5">code</a></li>
							<li class="nav-item"><a class="pure-button" href="http://github.com/timm/l5/issues">issues</a></li>
							<li class="nav-item"><a class="pure-button" href="http://github.com/timm/l5">docs</a></li>
							<li class="nav-item"><a class="pure-button" href="http://github.com/timm/l5">install</a></li>
						</ul>
					</nav>
				</div>
			</div>
			<div class="content pure-u-1 pure-u-md-3-4">
				<div>
					<!-- A wrapper for all the blog posts -->
					<div class="posts">


<p><img align=right width=175 src=head.png></p>
<p><strong><a href="https://menzies.us/l5">Docs</a> •  <a href="http://github.com/timm/l5.LICENSE.md">&copy;2022</a>.</strong> Tim Menzies</p>
<h1 id="do-you-understand-ai">Do you understand AI?</h1>
<p>How well do  you understand AI? 
Do you want to understand it better?  Want to practice that skill, a little more?</p>
<p>If you answering "yes", here's a little exercise for you.</p>
<ul>
<li>Step1: List out down what you think an AI toolkit should do.
Don't say "deep learning" or "clustering"-- those are just solution technologies.
Think instead of what services people want to get out of an AI. Things like
"watch for problems" or  "help me debate ideas with my friends" or "learn a summary
of what is going on right now" or "solutions we can trust".</li>
<li>Step2:  Now,  code 
 up those services, as succinctly as can (say, in less than 1000 lines of code). Try to find
 patterns in the processing, such all the parts of your code are used
 in as many different ways as possible,
 And no cheating 
(so no importing of some massive background AI library).</li>
</ul>
<p>If you try this, and show  your list to someone else, straight away you will learn that
"AI" is not one thing. Different people will list different things, depending on their
training and experiences. For the record, I'm all about using AI to help people understand
the world around them so my "AI" tries to find useful and succinct symbolic summaries. Other
people might prefer (e.g.)  sub-symbolic neural approaches. No worries, we can still be friends. And
maybe when those folks want succinct explanations of the systems they are exploring,
they might remember this code. </p>
<p>But lets get to some specifics.
Buse and Zimmermann<sup id="fnref:Bu12"><a class="footnote-ref" href="#fn:Bu12">3</a></sup> surveyed over  hundreds of developers and managers to
find a list of "information needs" for software analytics. 
Lets make sure you understand these, enough, to code them succinctly.</p>
<p><img alt="bi.png" src="bi.png" /></p>
<p>(Aside: this list is not as complete as I'd like.
current concerns about FAT (fair, accountable, trust) or bias
mitigation.
Nor does it really touch on knowledge acquisition or
or how to trade
off goals between competing stakeholders.
But that's ok-- we can add that i and still not break
1000 lines of code).</p>
<p>I use this exercise  to teach software engineers what goes on inside AI.
My premise is that
the AI-literate engineering should be able to mix and match AI tools
  to create specific solutions for specific problems.<br />
To show then what goes on inside the box,
I ask them to:</p>
<ul>
<li>Reproduce this code in whatever language they like (except the one used here). That takes 6-8 weeks.</li>
<li>Find all the short-cuts in this code, then find other AI tools that another approach to those short-cuts.</li>
<li>Benchmark this tiny toolkit against those more elaborate tools. This is end-of-semester project, which takes
  another 6-8 weeks.</li>
</ul>
<h2 id="why-is-all-these-coded-in-lua">Why is all these coded in LUA?</h2>
<ul>
<li>Because Lua is fun to write and <a href="https://learnxinyminutes.com/lua/">easy to learn</a>.</li>
<li>Because I want you to learn AI by coding it up from scratch, but I do not want to 
  give you a fully worked solution.  So here's my code-- and your job is to recode it.</li>
<li>Because LUA is great for teaching since 
  it installs, very quickly, on most platforms. This means that 
  this code can serve as an executable specification
  that students can use to check the output of their own code.</li>
<li>Because LUA supports multiple programming methods, including procedural, 
  object-oriented, functional, and data-driven programming. You can also use it to
  write you own <a href="https://www.lua.org/wshop11/luaws11_ag.pdf">domain-specific alnguages</a>.</li>
<li>Also, for hyperparameter optimziation, LUA has an interesting special advantage</li>
<li>A LUA name space is a very simple, very regular thing.
    If I load  a file twice (using <code>dofile</code>) then I get <strong>two</strong> copies of the namespace of that code. </li>
<li>Which means I can write an optimizer in LUA
    and use that optimizer to optimize itself (in another namespace) 
    which almost no chance (*)  of variables  in one space messing with the other</li>
</ul>
<p>(*) Of course, nothing is 100% safe. If one namespace reset the random number seed, 
that change can spread to the other space. Ditto with any other
global defined in the background LUA libraries.</p>
<p><br clear=all><hr></p>
<p>The need for baselines. XXXX</p>
<p>Standard supervised learners assume that all examples have labels.
When this is not true, then we need tools to incrementally 
(a) summarize what has been seen so far; (b) find and focus
on the most interesting part of that summary, (c) collect
more data in that region, then (d) repeat.</p>
<p><a href="div.png"><img align=right width=225 src="div.png"></a>
To make that search manageable, it is useful to exploit a 
manifold assumption; i.e.
higher-dimensional data can be approximated in a lower dimensional
manifold without loss of signal<sup id="fnref:Ch05"><a class="footnote-ref" href="#fn:Ch05">4</a></sup> <sup id="fnref:Le05"><a class="footnote-ref" href="#fn:Le05">8</a></sup>.
Manifolds lead to <em>continuity</em>
effects; i.e. if there are fewer dimensions, then there are more
similarities between examples.
Continuity simplifies <em>clustering</em>
(<a href="and">and</a> any subsequent reasoning).  More similarities means  easier
clustering. And after clustering, reasoning just means reason about
a handful of examples (maybe even just one)  from each cluster.</p>
<p><strong>HOMEWORKS</strong></p>
<ul>
<li><strong>Scripting</strong>: little languages (e.g. regular expressions); test-drive-development, CLI design, version control, GitHub workflows, test-driven development, open science</li>
<li>Data layer: summarize text files as samples of data</li>
<li>Anomaly detection: Nearest-neighbor, Clustering</li>
<li>Validation: effect size, significance texts</li>
<li>Semi-supervised learning</li>
<li>Regression, model trees.</li>
<li>Discretization, explanation</li>
<li>Privacy</li>
<li>Planning, monitoring</li>
<li>Optimization</li>
<li>Bias, Fairness mitigation</li>
</ul>
<p><strong>ASSiGNMENTS</strong></p>
<ul>
<li><strong>Instance selection</strong>: filter the data down to just a few samples per
cluster, the reason using just those.</li>
<li><strong>Anomaly detection</strong>:</li>
<li><strong>Explanation</strong> 
Discretize the numeric ranges (*) at each level of the recursion,
then divide the data according what range best selects for one half, or the other
at the data at this level of recursion.</li>
<li><strong>Multi-objective optimization:</strong> This code
can apply Zitzler's multi-objective ranking predicate<sup id="fnref:Zit04"><a class="footnote-ref" href="#fn:Zit04">11</a></sup> to prune the worst
half of the data, then recurs on the rest<sup id="fnref:Ch18"><a class="footnote-ref" href="#fn:Ch18">5</a></sup>. Assuming a large over-generation
of the initial population (to say, 10,000, examples), this can be just as effective
as genetic optimization<sup id="fnref2:Ch18"><a class="footnote-ref" href="#fn:Ch18">5</a></sup>, but runs much faster.</li>
<li><strong>Semi-supervised learning</strong>: these applications require only the <em>2.log(N)</em> labels at
of the pair of furthest points seen at each level of recursion.</li>
<li><strong>Privacy</strong></li>
<li><strong>Planning</strong></li>
<li><strong>Monitoring</strong></li>
</ul>
<h2 id="toc">TOC</h2>
<ul>
<li>Some theory</li>
<li>nothing as useful as a good theory</li>
<li>kelly. understand the world via . cluster and contrast<ul>
<li>unifnying framework srteching for initilaiz conceptualization internal to a sysrtem</li>
</ul>
</li>
<li>constrasts<ul>
<li>difference between things shorter than the things</li>
</ul>
</li>
<li>super discretization<ul>
<li>consider everything we want for good and bad cluster. </li>
<li>tabu, plan, monitor, optimization , explain</li>
<li>all ways to assess ranges</li>
<li>repeated observastion: a few ranges are most powerful</li>
<li>so when we say cluster and contrast, we mean find the ranges that most
  distinguish things</li>
<li>of course we combine these ranges into model... but we might not have to</li>
<li>could just print_ the super ranges and stop</li>
</ul>
</li>
<li>chi merge: botton up discretization to combine silimar ranges</li>
<li>fastmap (an approximation to pca)</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:Ah91">
<p>Aha, D.W., Kibler, D. &amp; Albert, M.K. <a href="https://link.springer.com/content/pdf/10.1007%2FBF00153759.pdf">Instance-based   learning algorithms</a>. Mach Learn 6, 37–66 (1991).  https://doi.org/10.1007/BF00153759&#160;<a class="footnote-backref" href="#fnref:Ah91" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:Bo98">
<p>Boley, D., 1998.  <a href="https://www-users.cse.umn.edu/~boley/publications/papers/PDDP.pdf">Principal directions divisive partitioning</a> Data Mining and Knowledge Discovery, 2(4): 325-344.&#160;<a class="footnote-backref" href="#fnref:Bo98" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:Bu12">
<p>Buse, R. and Zimmerman T. <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2011-8.pdf">Information Needs for Software Development Analytics</a>, ICSE'12, 2012&#160;<a class="footnote-backref" href="#fnref:Bu12" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:Ch05">
<p><a href="http://www.molgen.mpg.de/3659531/MITPress--SemiSupervised-Learning">Semi-Supervised Learning</a> (2005) Olivier Chapelle,  Bernhard Schölkopf, and Alexander Zien (eds).  MIT Press.&#160;<a class="footnote-backref" href="#fnref:Ch05" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:Ch18">
<p><a href="https://arxiv.org/pdf/1608.07617.pdf">Sampling” as a Baseline Optimizer for Search-Based Software Engineering</a>, Jianfeng Chen; Vivek Nair; Rahul Krishna; Tim Menzies IEEE Trans SE, (45)6, 2019&#160;<a class="footnote-backref" href="#fnref:Ch18" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:Ch18" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:Ch22">
<p><a href="https://arxiv.org/pdf/2111.02038.pdf">Can We Achieve Fairness Using Semi-Supervised Learning?</a> (2022), Joymallya Chakraborty, Huy Tu, Suvodeep Majumder, Tim Menzies.&#160;<a class="footnote-backref" href="#fnref:Ch22" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="fn:Fal95">
<p>Christos Faloutsos and King-Ip Lin. 1995. FastMap: a fast algorithm for indexing, data-mining and visualization of traditional and multimedia datasets. SIGMOD Rec. 24, 2 (May 1995), 163–174. DOI:https://doi.org/10.1145/568271.223812&#160;<a class="footnote-backref" href="#fnref:Fal95" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="fn:Le05">
<p>Levina, E., Bickel, P.J.: <a href="https://www.stat.berkeley.edu/~bickel/mldim.pdf">Maximum likelihood estimation of intrinsic dimension</a>.  In: Advances in neural information processing systems, pp. 777–784 (2005)&#160;<a class="footnote-backref" href="#fnref:Le05" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
<li id="fn:Ke92">
<p>Kerber, Randy <a href="https://www.aaai.org/Papers/AAAI/1992/AAAI92-019.pdf">ChiMerge: Discretization of Numeric Attributes</a>, AAAI'92&#160;<a class="footnote-backref" href="#fnref:Ke92" title="Jump back to footnote 9 in the text">&#8617;</a></p>
</li>
<li id="fn:Pl04">
<p>Platt, John.  <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2005/01/nystrom2.pdf">FastMap, MetricMap, and Landmark MDS are all Nystrom Algorithms</a> AISTATS (2005).&#160;<a class="footnote-backref" href="#fnref:Pl04" title="Jump back to footnote 10 in the text">&#8617;</a></p>
</li>
<li id="fn:Zit04">
<p><a href="https://link.springer.com/chapter/10.1007/978-3-540-30217-9_84">Indicator-based selection in multiobjective search</a> Eckart Zitzler , Simon Künzli Proc. 8th International Conference on Parallel Problem Solving from Nature (PPSN VIII&#160;<a class="footnote-backref" href="#fnref:Zit04" title="Jump back to footnote 11 in the text">&#8617;</a></p>
</li>
</ol>
</div>

					</div>
				</div>
				<div class="footer">
					<div class="pure-menu pure-menu-horizontal">
						<ul>
							<li class="pure-menu-item"><a href="http://purecss.io/" class="pure-menu-link">About</a></li>
							<li class="pure-menu-item"><a href="http://github.com/pure-css/pure/" class="pure-menu-link">GitHub</a></li>
						</ul>
					</div>
				</div>
			</div>
		</div>
	</body>
</html>
